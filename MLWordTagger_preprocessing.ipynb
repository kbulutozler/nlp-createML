{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea216b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1610bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets loaded to dataframes in  3.8040120601654053  seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "df_tags = pd.read_csv(\"pos_tags.csv\") # this data is from https://www.kaggle.com/atmarouane/en-partofspeech-tags\n",
    "                                      # the tokens this data refers to is stored in below csv file\n",
    "                                      # this data has tags for tokens that have \"plain\" class in below csv file\n",
    "df_tokens = pd.read_csv(\"en_train.csv\") # this data is from https://www.kaggle.com/c/text-normalization-challenge-english-language\n",
    "                                        # this data has tokens in both normalized and unnormalized form\n",
    "                                        # for the importance of normalization, visit above link\n",
    "                                        # this notebook creates json file for word tagger by createML\n",
    "                                        # since dataset is very large, i will create json files consist of 15000 sentences\n",
    "print(\"datasets loaded to dataframes in \", time.time() - t0, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543f54a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed sentences:  500 duration from start:  126.66433501243591  seconds\n",
      "processed sentences:  1000 duration from start:  252.1863079071045  seconds\n",
      "processed sentences:  1500 duration from start:  379.21092772483826  seconds\n",
      "processed sentences:  2000 duration from start:  512.0134110450745  seconds\n",
      "processed sentences:  2500 duration from start:  641.4872167110443  seconds\n",
      "processed sentences:  3000 duration from start:  766.3885598182678  seconds\n",
      "processed sentences:  3500 duration from start:  890.1617217063904  seconds\n",
      "processed sentences:  4000 duration from start:  1010.5375699996948  seconds\n",
      "processed sentences:  4500 duration from start:  1135.7369179725647  seconds\n",
      "processed sentences:  5000 duration from start:  1261.3873648643494  seconds\n",
      "processed sentences:  5500 duration from start:  1385.1812028884888  seconds\n",
      "processed sentences:  6000 duration from start:  1521.6367616653442  seconds\n",
      "processed sentences:  6500 duration from start:  1649.8430540561676  seconds\n",
      "processed sentences:  7000 duration from start:  1783.4631190299988  seconds\n",
      "processed sentences:  7500 duration from start:  1911.8047289848328  seconds\n",
      "processed sentences:  8000 duration from start:  2050.6274218559265  seconds\n",
      "processed sentences:  8500 duration from start:  2172.317503929138  seconds\n",
      "processed sentences:  9000 duration from start:  2301.282770872116  seconds\n",
      "processed sentences:  9500 duration from start:  2430.3958897590637  seconds\n",
      "processed sentences:  10000 duration from start:  2556.65127491951  seconds\n",
      "processed sentences:  10500 duration from start:  2684.968263864517  seconds\n",
      "processed sentences:  11000 duration from start:  2809.2710309028625  seconds\n",
      "processed sentences:  11500 duration from start:  2936.2372658252716  seconds\n",
      "processed sentences:  12000 duration from start:  3062.2499828338623  seconds\n",
      "processed sentences:  12500 duration from start:  3185.8389208316803  seconds\n",
      "processed sentences:  13000 duration from start:  3305.790543079376  seconds\n",
      "processed sentences:  13500 duration from start:  3433.412381887436  seconds\n",
      "processed sentences:  14000 duration from start:  3560.476310968399  seconds\n",
      "processed sentences:  14500 duration from start:  3688.800535917282  seconds\n",
      "processed sentences:  15000 duration from start:  3811.901039838791  seconds\n",
      "lists for json files have been built in  3811.9040019512177  seconds\n",
      "processed sentences:  15001\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "unnormalized_list_to_json = []\n",
    "normalized_list_to_json = []\n",
    "sentence_ctr = 0\n",
    "last_sentence_id = df_tokens.tail(1).iloc[0][\"sentence_id\"] # too much\n",
    "last_sentence_id = 15000 \n",
    "while sentence_ctr <= last_sentence_id:\n",
    "    unnormalized_sentence_tokens = []\n",
    "    unnormalized_sentence_labels = []\n",
    "    unnormalized_sentence_dict = {}\n",
    "    normalized_sentence_tokens = []\n",
    "    normalized_sentence_labels = []\n",
    "    normalized_sentence_dict = {}\n",
    "    \n",
    "    \n",
    "    for index, row in df_tokens[df_tokens[\"sentence_id\"] == sentence_ctr].iterrows():\n",
    "        sentence_id = row[\"sentence_id\"]\n",
    "        token_id = row[\"token_id\"]\n",
    "        label = \"NONE\"\n",
    "        if(sentence_id in df_tags[\"sentence_id\"].values):\n",
    "            if(token_id in df_tags[df_tags[\"sentence_id\"] == sentence_id][\"token_id\"].values):\n",
    "                label = df_tags[\"pos\"][(df_tags[\"sentence_id\"] == sentence_id) & (df_tags[\"token_id\"] == token_id)].iloc[0]\n",
    "                if(type(label) != str): # all labels other than str is float \n",
    "                    if(math.isnan(label)): # all float labels are float('nan')\n",
    "                        label = \"None\"      \n",
    "        \n",
    "        unnormalized = row[\"before\"]\n",
    "        if(type(unnormalized) == str):\n",
    "            for token in unnormalized.split(' '):\n",
    "                if(len(token) > 0):\n",
    "                    unnormalized_sentence_tokens.append(token)\n",
    "                    unnormalized_sentence_labels.append(label)\n",
    "        else:\n",
    "            unnormalized_sentence_tokens.append(str(unnormalized))\n",
    "            unnormalized_sentence_labels.append(label)\n",
    "           \n",
    "        normalized = row[\"after\"]\n",
    "        if(type(normalized) == str):\n",
    "            for token in normalized.split(' '):\n",
    "                if(len(token) > 0):\n",
    "                    normalized_sentence_tokens.append(token)\n",
    "                    normalized_sentence_labels.append(label)\n",
    "        else:\n",
    "            normalized_sentence_tokens.append(str(normalized))\n",
    "            normalized_sentence_tokens.append(label)\n",
    "    \n",
    "    if(len(unnormalized_sentence_tokens) != 0):\n",
    "        unnormalized_sentence_dict[\"tokens\"] = unnormalized_sentence_tokens\n",
    "        unnormalized_sentence_dict[\"labels\"] = unnormalized_sentence_labels\n",
    "        unnormalized_list_to_json.append(unnormalized_sentence_dict)\n",
    "    \n",
    "    if(len(normalized_sentence_tokens) != 0):\n",
    "        normalized_sentence_dict[\"tokens\"] = normalized_sentence_tokens\n",
    "        normalized_sentence_dict[\"labels\"] = normalized_sentence_labels\n",
    "        normalized_list_to_json.append(normalized_sentence_dict)\n",
    "    \n",
    "    if(sentence_ctr%500 == 0 and sentence_ctr!=0):\n",
    "        print(\"processed sentences: \", sentence_ctr, \"duration from start: \", time.time() - t0, \" seconds\")\n",
    "    sentence_ctr += 1\n",
    "       \n",
    "    \n",
    "print(\"lists for json files have been built in \", time.time() - t0, \" seconds\")\n",
    "print(\"processed sentences: \", sentence_ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831468f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(normalized_list_to_json):\n",
    "    for token in entry[\"tokens\"]:\n",
    "        if(type(token) is not str):\n",
    "            print(token)\n",
    "    for label in entry[\"labels\"]:\n",
    "        if(type(label) is not str):\n",
    "            print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff8f1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(unnormalized_list_to_json):\n",
    "    for token in entry[\"tokens\"]:\n",
    "        if(type(token) is not str):\n",
    "            print(token)\n",
    "    for label in entry[\"labels\"]:\n",
    "        if(type(label) is not str):\n",
    "            print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e26c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unnormalized.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unnormalized_list_to_json, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e6885bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('normalized.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(normalized_list_to_json, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e615a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unnormalized_train_6k.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unnormalized_list_to_json[0:6000], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7513244",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('normalized_train_6k.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(normalized_list_to_json[0:6000], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0b098bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unnormalized_train_12k.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unnormalized_list_to_json[0:12000], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75d2493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('normalized_train_12k.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(normalized_list_to_json[0:12000], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30eec9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unnormalized_test_3k.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unnormalized_list_to_json[12000:], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10416ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('normalized_test_3k.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(normalized_list_to_json[12000:], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba6726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
