{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ea216b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de1610bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets loaded to dataframes in  3.933832883834839  seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "df_tags = pd.read_csv(\"pos_tags.csv\") # this data is from https://www.kaggle.com/atmarouane/en-partofspeech-tags\n",
    "                                      # the tokens this data refers to is stored in below csv file\n",
    "                                      # this data has tags for tokens that have \"plain\" class in below csv file\n",
    "df_tokens = pd.read_csv(\"en_train.csv\") # this data is from https://www.kaggle.com/c/text-normalization-challenge-english-language\n",
    "                                        # this data has tokens in both normalized and unnormalized form\n",
    "                                        # for the importance of normalization, visit above link\n",
    "                                        # this notebook creates json file for word tagger by createML\n",
    "                                        # i will create 2 json files: normalized tokens-tags and unnormalized tokens-tags\n",
    "                                        # since dataset is very large, i will create json files consist of 15000 sentences\n",
    "print(\"datasets loaded to dataframes in \", time.time() - t0, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "543f54a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed sentences:  500 duration from start:  128.3681080341339  seconds\n",
      "processed sentences:  1000 duration from start:  254.97327995300293  seconds\n",
      "processed sentences:  1500 duration from start:  384.57196593284607  seconds\n",
      "processed sentences:  2000 duration from start:  520.2205719947815  seconds\n",
      "processed sentences:  2500 duration from start:  652.4947261810303  seconds\n",
      "processed sentences:  3000 duration from start:  779.7631080150604  seconds\n",
      "processed sentences:  3500 duration from start:  905.4591898918152  seconds\n",
      "processed sentences:  4000 duration from start:  1025.8955838680267  seconds\n",
      "processed sentences:  4500 duration from start:  1152.4406352043152  seconds\n",
      "processed sentences:  5000 duration from start:  1279.099709033966  seconds\n",
      "processed sentences:  5500 duration from start:  1404.2063348293304  seconds\n",
      "processed sentences:  6000 duration from start:  1538.541561126709  seconds\n",
      "processed sentences:  6500 duration from start:  1666.6410548686981  seconds\n",
      "processed sentences:  7000 duration from start:  1799.6142151355743  seconds\n",
      "processed sentences:  7500 duration from start:  1929.124279975891  seconds\n",
      "processed sentences:  8000 duration from start:  2060.903179883957  seconds\n",
      "processed sentences:  8500 duration from start:  2183.842178106308  seconds\n",
      "processed sentences:  9000 duration from start:  2313.9075100421906  seconds\n",
      "processed sentences:  9500 duration from start:  2445.547247171402  seconds\n",
      "processed sentences:  10000 duration from start:  2573.5335762500763  seconds\n",
      "processed sentences:  10500 duration from start:  2702.2242600917816  seconds\n",
      "processed sentences:  11000 duration from start:  2826.4897451400757  seconds\n",
      "processed sentences:  11500 duration from start:  2954.13090801239  seconds\n",
      "processed sentences:  12000 duration from start:  3081.9272871017456  seconds\n",
      "processed sentences:  12500 duration from start:  3206.250859975815  seconds\n",
      "processed sentences:  13000 duration from start:  3328.1110582351685  seconds\n",
      "processed sentences:  13500 duration from start:  3457.1713211536407  seconds\n",
      "processed sentences:  14000 duration from start:  3585.6911709308624  seconds\n",
      "processed sentences:  14500 duration from start:  3714.995605945587  seconds\n",
      "processed sentences:  15000 duration from start:  3838.674464941025  seconds\n",
      "lists for json files have been built in  3838.6746079921722  seconds\n",
      "processed sentences:  15001\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "unnormalized_list_to_json = []\n",
    "normalized_list_to_json = []\n",
    "sentence_ctr = 0\n",
    "last_sentence_id = df_tokens.tail(1).iloc[0][\"sentence_id\"] # too much\n",
    "last_sentence_id = 15000 \n",
    "while sentence_ctr <= last_sentence_id:\n",
    "    unnormalized_sentence_tokens = []\n",
    "    unnormalized_sentence_labels = []\n",
    "    unnormalized_sentence_dict = {}\n",
    "    normalized_sentence_tokens = []\n",
    "    normalized_sentence_labels = []\n",
    "    normalized_sentence_dict = {}\n",
    "    \n",
    "    \n",
    "    for index, row in df_tokens[df_tokens[\"sentence_id\"] == sentence_ctr].iterrows():\n",
    "        sentence_id = row[\"sentence_id\"]\n",
    "        token_id = row[\"token_id\"]\n",
    "        label = \"NONE\"\n",
    "        if(sentence_id in df_tags[\"sentence_id\"].values):\n",
    "            if(token_id in df_tags[df_tags[\"sentence_id\"] == sentence_id][\"token_id\"].values):\n",
    "                label = df_tags[\"pos\"][(df_tags[\"sentence_id\"] == sentence_id) & (df_tags[\"token_id\"] == token_id)].iloc[0]\n",
    "                if(type(label) != str): # all labels other than str is float \n",
    "                    if(math.isnan(label)): # all float labels are float('nan')\n",
    "                        label = \"None\"      \n",
    "        \n",
    "        unnormalized = row[\"before\"]\n",
    "        if(type(unnormalized) == str):\n",
    "            for token in unnormalized.split(' '):\n",
    "                if(len(token) > 0):\n",
    "                    unnormalized_sentence_tokens.append(token)\n",
    "                    unnormalized_sentence_labels.append(label)\n",
    "        else:\n",
    "            unnormalized_sentence_tokens.append(str(unnormalized))\n",
    "            unnormalized_sentence_labels.append(label)\n",
    "           \n",
    "        normalized = row[\"after\"]\n",
    "        if(type(normalized) == str):\n",
    "            for token in normalized.split(' '):\n",
    "                if(len(token) > 0):\n",
    "                    normalized_sentence_tokens.append(token)\n",
    "                    normalized_sentence_labels.append(label)\n",
    "        else:\n",
    "            normalized_sentence_tokens.append(str(normalized))\n",
    "            normalized_sentence_tokens.append(label)\n",
    "    \n",
    "    if(len(unnormalized_sentence_tokens) != 0):\n",
    "        unnormalized_sentence_dict[\"tokens\"] = unnormalized_sentence_tokens\n",
    "        unnormalized_sentence_dict[\"labels\"] = unnormalized_sentence_labels\n",
    "        unnormalized_list_to_json.append(unnormalized_sentence_dict)\n",
    "    \n",
    "    if(len(normalized_sentence_tokens) != 0):\n",
    "        normalized_sentence_dict[\"tokens\"] = normalized_sentence_tokens\n",
    "        normalized_sentence_dict[\"labels\"] = normalized_sentence_labels\n",
    "        normalized_list_to_json.append(normalized_sentence_dict)\n",
    "    \n",
    "    if(sentence_ctr%500 == 0 and sentence_ctr!=0):\n",
    "        print(\"processed sentences: \", sentence_ctr, \"duration from start: \", time.time() - t0, \" seconds\")\n",
    "    sentence_ctr += 1\n",
    "       \n",
    "    \n",
    "print(\"lists for json files have been built in \", time.time() - t0, \" seconds\")\n",
    "print(\"processed sentences: \", sentence_ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "831468f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(normalized_list_to_json):\n",
    "    for token in entry[\"tokens\"]:\n",
    "        if(type(token) is not str):\n",
    "            print(token)\n",
    "    for label in entry[\"labels\"]:\n",
    "        if(type(label) is not str):\n",
    "            print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "02051bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_list_to_json_12k = unnormalized_list_to_json[:12000]\n",
    "normalized_list_to_json_12k = normalized_list_to_json[:12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "588b9c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(normalized_list_to_json)\n",
    "random.shuffle(unnormalized_list_to_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb79d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "average length = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "33e26c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unnormalized.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unnormalized_list_to_json, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4e6885bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('normalized.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(normalized_list_to_json, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d0ce38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unnormalized.json', 'r') as f:\n",
    "    data=f.read()\n",
    "unnormalized_json_to_list = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5a41df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('normalized.json', 'r') as f:\n",
    "    data=f.read()\n",
    "normalized_json_to_list = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e615a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unnormalized_train_12000.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unnormalized_json_to_list[0:12000], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f7513244",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('normalized_train_12000.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unnormalized_json_to_list[0:12000], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4f5835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unnormalized_train_10k.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unnormalized_json_to_list[0:10000], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11474649",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('normalized_train_10k.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unnormalized_json_to_list[0:10000], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0b098bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unnormalized_train_20k.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unnormalized_json_to_list[0:20000], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75d2493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('normalized_train_20k.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unnormalized_json_to_list[0:20000], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "30eec9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unnormalized_test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(unnormalized_list_to_json[12000:], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "10416ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('normalized_test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(normalized_list_to_json[12000:], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f3d23da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, pair in enumerate(unnormalized_json_to_list):\n",
    "    if(len(pair[\"tokens\"]) == 0):\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48c275d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>class</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>DATE</td>\n",
       "      <td>August 16, 2005</td>\n",
       "      <td>august sixteenth two thousand five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id  token_id  class           before  \\\n",
       "513           41         0   DATE  August 16, 2005   \n",
       "514           41         1  PUNCT                .   \n",
       "\n",
       "                                  after  \n",
       "513  august sixteenth two thousand five  \n",
       "514                                   .  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens[df_tokens[\"sentence_id\"]==41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d85a73c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "41 in df_tags[\"sentence_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fde0c021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token_id</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>NNPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id  token_id   pos\n",
       "370           40         0   NNP\n",
       "371           40         4   NNP\n",
       "372           40         5   NNP\n",
       "373           40         6  NNPS\n",
       "374           40         8    NN\n",
       "375           40        10   NNP"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tags[df_tags[\"sentence_id\"]==40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "38f90949",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(unnormalized_list_to_json):\n",
    "    for token in entry[\"labels\"]:\n",
    "        if(type(token) is not str and type(token) is not float):\n",
    "            print(token)\n",
    "            print(type(token))\n",
    "            print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b2bad16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in df_tokens[\"before\"].values:\n",
    "    if(type(value) is float and not math.isnan(value)):\n",
    "        print(value)\n",
    "        print(type(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "01c93895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dbe9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
